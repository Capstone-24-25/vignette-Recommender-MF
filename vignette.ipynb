{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Recommendor Systems with Deep Learning - Matrix Factorization\"\n",
        "author: \"Kuan-I Lu, Colin Nguyen, Candis Wu, Caitlyn Vasquez, Carter Kulm\"\n",
        "date: \"`r Sys.Date()`\"\n",
        "format: html\n",
        "---"
      ],
      "id": "23ff5d6f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Overview\n",
        "Our vignette is on reccomender systems that utilize deep learning techniques.  Reccomendation systems are a tool that allow us to analyze interactions between users and items, leveraging historical data to predict future interactions.  There are two types of reccomendations systems, content-based and colloboaring filtering, which is what are using.  In collaborative filtering, the system predicts user preferences based off the behavior of oother users, assuming that users with similar past preferences will have similar future preferences.  Furthermore, advanced reccomendation systems can incorperate machine learning and deep learning techniques to improve accuracy, allowing the system to capture complex on linear relationships in the data.\n",
        "\n",
        "# Data Description\n",
        "\n",
        "This dataset is sourced from Kaggle after searching for data that matched the structure of examples referenced in the Dive Into Deep Learning textbook. The dataset contains user ratings for products, making it a strong candidate for testing recommendation systems, particularly collaborative filtering models.\n",
        "\n",
        "The dataset is part of the Amazon Reviews Data repository, which was curated by Julian McAuley. It contains product reviews and ratings across multiple categories. For our project, we specifically utilized the Electronics dataset. The original data source can be found [here](https://www.kaggle.com/datasets/saurav9786/amazon-product-reviews).\n",
        "\n",
        "Note: In our data folder we will only include 5000 rows of data for reference because the entire data file is too large to be uploaded on github. \n",
        "\n",
        "\n",
        "### Attribute information: \\\n",
        "-   `userId` : Every user identified with a unique id (First Column)\\\n",
        "-   `productId` : Every product identified with a unique id (Second Column)\\\n",
        "-   `Rating` : Rating of the corresponding product by the corresponding user (Third Column)\\\n",
        "-   `timestamp` : Time of the rating (Fourth Column)\\\n",
        "\n",
        "# Matrix Factorization\n",
        "\n",
        "With the cleaned data, we can construct the sparse user-item matrix containing information of user's ratings, with unique users as unique rows and unique items as unique columns. The problem with this matrix is that it's too large, and the goal of imputing the missing entries (the unrated items by each user) can be computationally expensive. To tackle this issue, the common approach is to factorize the matrix into two latent matrices, one explaining the characters of users, another explaining the characters of different items. Figure 3 shows the visualization of matrix factorization, given a $m \\times n$ matrix, we want to create a $m \\times k$ user matrix and a $k \\times n$ matrix, with $k << m, n$ so that the product of the user and item matrix is a good estimate of the observed user-item matrix. \n",
        "\n",
        "(graph here)\n",
        "\n",
        "There are several methods to approach this, one being simply applying Singular Value Decomposition (SVD) and obtaining the user and item matrix directly. This is going to be our benchmark. A more advanced approach is to implement deep learning algorithms to train for the weights (parameters) in the user and item matrix, respectively. To make the product of these two matrices having values close to the observed matrix.\n",
        "\n",
        "\n",
        "\n",
        "# Baseline Model(SVD)\n",
        "\n",
        "Singular value decomposition (or SVD) is a method of matrix factorization that consists of rescaling and multiple rotations that eventually results in three components: U, a unitary matrix consisting of left singular vectors, ∑, a rectangular matrix consisting of eigenvectors on the diagonal, and V, a complex unitary matrix with right singular vectors. To implement SVD in Python we will use the functions drawn from the scikit-surprise package. We will first specify a reader() object which will help to parse our input data.\n"
      ],
      "id": "09d884ff"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from surprise import SVD, Dataset, Reader, accuracy\n",
        "from surprise.model_selection import train_test_split\n",
        "reader = Reader(rating_scale=(1, 5))\n",
        "data = Dataset.load_from_df(data[['user_id', 'item_id', 'rating']], reader)"
      ],
      "id": "b7adc8b2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then create our training and testing sets in addition to initializing our svd model. \n"
      ],
      "id": "93ed7068"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "trainset, testset = train_test_split(data, test_size=0.2)\n",
        "svd = SVD()"
      ],
      "id": "2e8e88d0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we compute predictions and analyze the model’s accuracy using RMSE and MAE as our evaluation metrics. \n"
      ],
      "id": "d216b821"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "predictions = svd.test(testset)\n",
        "rmse = accuracy.rmse(predictions)\n",
        "mae = accuracy.mae(predictions)"
      ],
      "id": "dd49609a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "RMSE: 1.2943\n",
        "MAE: 1.0190\n",
        "\n",
        "Thus we see that our baseline model, SVD, didn't perform horribly in terms of RMSE and MAE. \n",
        "\n",
        "# Deep Learning"
      ],
      "id": "dbcbfd3b"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}